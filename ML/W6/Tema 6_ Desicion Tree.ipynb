{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPT1qOWhiHWrEMCtKAEK72E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[" # Tema 6: Decision Tree\n"," ----\n","  \n","University: UTEC \\\\\n","   Course: Machine Learning \\\\\n","   Professor: Cristian López Del Alamo \\\\\n","   Topic: Decision Trees \\\\\n","  \n","\n","  ----\n","\n","  Names and Surnames of Members: (Do not forget to enter the % participation)\n","  - Member 1:\n","  - Member 2:\n","  - Member 3:\n","  - Member 4:\n","  \n","\n"],"metadata":{"id":"9lQct68MlKo0"}},{"cell_type":"markdown","source":["For this exercise, you will need to build your own decision tree.\n","\n","- Please note that this is a recursive algorithm.\n","- The base case occurs when all the elements of a node have the same labels, that is, it is a terminal node. Then the label of that node takes the value of the common label.\n","- In the case that it is not a terminal node, the algorithm must search for one of the\n","feactures by which to divide and for this use Information Gain (Entropy or Gini).\n","- Split the dataset using the feature that generates a greater information gain in the parent or a lower GINI and recursively call the create_DT function.\n","\n","You will use the iris database, with 4 features and 3 classes.\n","Randomly take 80% of the data to create the tree and the rest to\n","test the accuracy of the prediction.\n","Finally, it shows the **accuracy** of its model using a confusion matrix.\n","\n","----\n","\n","Download the [Dataset](https://gist.github.com/Thanatoz-1/9e7fdfb8189f0cdf5d73a494e4a6392a)\n","\n","----\n","\n","\n","Work in teams:\n","\n","[Documento to help 1](https://towardsdatascience.com/the-simple-math-behind-3-decision-tree-splitting-criterions-85d4de2a75fe)\n","\n","[Documento to help 2](https://www.quantstart.com/articles/Beginners-Guide-to-Decision-Trees-for-Supervised-Machine-Learning/)\n","\n","----\n","## In the following code, your group can add functions, remove functions, or modify the function parameters.\n","---"],"metadata":{"id":"G19GO8Aflmlv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o74EDY9g86G9"},"outputs":[],"source":["import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","\n","iris = sns.load_dataset('iris')\n","print(iris.head())\n","\n","\n","class Nodo:\n"," #Define what your data members will be\n","  def __init__(index):\n","    # Initialize data members\n","    pass\n","\n","  def IsTerminal(self,Y):\n","    # return true if this node has the same labels in Y\n","    pass\n","\n","\n","  def BestSplit(self,X,Y):\n","    # write your code here\n","    pass\n","\n","  def Entropy(self,Y):\n","    # write your code here\n","    pass\n","\n","  def Gini(self,Y):\n","    # write your code here\n","    pass\n","\n","\n","\n","class DT:\n"," # Defina cuales será sus mimbros datos\n","   self.m_Root = None\n","\n","   def __init__(X, Y, index):\n","    # Inicializar los mimbros datos\n","\n","    pass\n","\n","   def create_DT(self):\n","      # write your code here\n","\n","   def Find_Best_Split(self):\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["Enter your team’s data and your collaboration her: [link ](https://docs.google.com/spreadsheets/d/1_PxoiuEgCAtnjQHSsyLOPYVXmyvd-pST9zZ6JNxWMNM/edit?usp=sharing)\n","\n","[Additional reading ](https://www.cs.toronto.edu/~axgao/cs486686_f21/lecture_notes/Lecture_07_on_Decision_Trees.pdf)"],"metadata":{"id":"A1mOS4oMjuhH"}}]}