# -*- coding: utf-8 -*-
"""Grupo 8_Tema_4_Regresion_Logistica_2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gfQHUq9QqtgWIN3p_oxDZNjYHVadDt6U

## Logistic Regression Practice: Breast Cancer
 ----
  University  : UTEC \\
Course      : Machine Learning \\
Professor   : Cristian López Del Alamo \\
Topic      : Logistic Regression \\
  
 ----


 Integrantes:
 - 1.  ($\; \% \;$)
 - 2.  ($\; \% \;$)
 - 3.  ($\; \% \;$)

# Dataset

---


The dataset contains cases from a study conducted between 1958 and 1970 at the Billings Hospital of the University of Chicago regarding the survival of patients who had undergone surgery for breast cancer.

The database consists of 306 objects, each object has 3 features (Patient's age at the time of the operation, Years of operation, and Number of positive axillary nodes detected) and one predictor (variable to predict survival status, 1 if the patient lived, 0 if the patient died).


---


<font color="blue!10">**Your task is to predict whether a patient will survive or not.**
</font>


## <font color="#FF6666"> Remember,  for every mistake your model makes, a possible death will weight on the shoulders of your team.</font>

**Download the Database**: [Click](https://docs.google.com/spreadsheets/d/137VWC-uXIeWUIy5F2oVkfPqicFeNFU5oUWVsh5om9a4/edit?usp=sharing)
"""

import numpy as np

"""

1. **Hypothesis**:

-  Line Equation or Hyperplane
\begin{equation}
h(x_i) = w_0 + w_1x_{i1} +  w_2x_{i2} ... w_kx_{ik}
\end{equation} \\

- In matrix form

\begin{equation}
h(x_i) = XW^t
\end{equation} \\





- Sigmoid Function Equation (Binary Classifier)
\begin{equation}
s(x_i) = \frac{1}{1 + e^{-h(x)}}
\end{equation}



---


Note: **Remember that X is a matrix where the columns represent the features, and the rows represent the number of elements in the database. Don’t forget to add a column of all 1s at the beginning of matrix X for the bias**

---

"""

def h(x,w):
    return np.dot(x,w.T)

def S(x,w):
    h_x = h(x,w)
    return 1/(1+np.exp(-h_x))

# x = np.array([[1,1,1],[1,1,1],[1,1,1]])
# w = np.array([1,2,3])

# print(S(x,w))

"""2  **Loss Function** (binary Cross-Entropy)

\begin{equation}
L = -\frac{1}{n}\sum_{i=0}^n(y_ilog(s(x_i)) + (1-y_i)log(1-s(x_i)))  
\end{equation} \\

"""

def Loss(y,y_aprox):
    n = len(y)
    e = 1e-10
    y_aprox = np.clip(y_aprox, e, 1 - e)
    return -1/n * np.sum(y * np.log(y_aprox) + (1-y)*np.log(1-y_aprox))

"""3 **Derivatives**

\begin{equation}
\frac{\partial L}{\partial w_j} = \frac{1}{n}\sum_{i=0}^n(y_i - s(x_i))(-x_{ij})
\end{equation} \\



---

Note: $x_{ij}$ refers to the $j_{esima}$ characteristic of the $i_{esimo}$    object in the training dataset


---



"""

def Derivatives(x,y,w):
    y_aprox = S(x, w)
    return np.dot((y_aprox - y).T, x)

"""4  Change parameters

\begin{equation}
 w_j = w_j - \alpha\frac{\partial L}{\partial w_j}
\end{equation}
"""

def change_parameters(w, derivatives, alpha):
    w = w - alpha * derivatives
    return w

"""## Bringing it all together : **Training** code




"""

def training(x,y, epochs, alpha):
    L_value = []
    n_feactures = x.shape[1]
    w = np.array([np.random.rand() for i in range(n_feactures)])
    print(w)
    for i in range(epochs):
        y_aprox = S(x,w)
        L =  Loss(y,y_aprox)
        dw = Derivatives(x,y,w)
        w =  change_parameters(w, dw, alpha)
        L_value.append(L)
    return L_value,w

x = np.array([[30,64,1],[30,62,3]])
y = np.array([0,1])

training(x,y,5,0.01)

"""## **Testing** code
Modify this function to return true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)



---


- TP: Percentage of elements predicted as class 1 (positive) that are actually class 1 (positive).
- TN: Percentage of elements predicted as class 0 (negative) and that are actually class 0 (negative).
- FP: Percentage of elements predicted as class 1 (positive) and that are actually class 0 (negative).
- FN: Percentage of elements predicted as class 0 (negative) and that are actually class 1 (positive).


---




"""

def Testing(x_test, y_test,w):
    n = len(y_test)
    y_pred = []
    for i in range(n):
        y_pred.append(S(x_test[i],w))
    print("Number of correct data : ", sum(y_pred == y_test))

"""
# 1 Loading Database
-  Don't forget to normalize data

"""

import pandas as pd
import numpy as np
from google.colab import files

filename = files.upload()
name = list(filename.keys())[0]
name = "db_cancer.csv"
data = pd.read_csv(name)
X = data[["C1","C2","C3"]]
Y = data[["Clase"]]
X.head()


#  normalize data: write your code here

"""
*   Randomly splitting data for training and testing
*   Training 70$\%$
*   Training 30$\%$

"""

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y , random_state=104,  test_size=0.30,    shuffle=True)

"""# 2. Training the Model

- Modify hyperparameters for better results
- Todos los datos de entrenamiento y testing estan como dataframe, deben convertirlos a tipo array para que todo funcione.




"""

import matplotlib.pyplot as plt

# Hyperparameters
epochs = 1000
alpha = 0.001
L,W = training(X_train, Y_train, epochs, alpha)
# Plotting the Loss Function vs Epochs
epochs_list = [i for i in range(epochs)]
plt.plot(epochs_list ,L)
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss vs Epochs")
plt.show()

"""# 3. Testing the model

"""

Testing(X_test, Y_test,W)

"""## Develop the following activities:
- Implement all the necessary functions for the code to work correctly.
- Try to use matrix operations to make your code more efficient.
-  Find the best hyperparameters.
- Calculate the percentage of patients correctly belonging to class 1 (true positives).
- Calculate the percentage of patients correctly belonging to class 0 (true negatives).
- Calculate the percentage of patients incorrectly belonging to class 1 (false positives).
- Calculate the percentage of patients incorrectly belonging to class 0 (false negatives).
- Show a table with true positives, true negatives, false positives, and false negatives

A very good book: [click](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)

Tips for manipulate pandas : https://towardsdatascience.com/python-pandas-vs-r-dplyr-5b5081945ccb
"""