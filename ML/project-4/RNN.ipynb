{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQM6jtNKwFZq"
   },
   "source": [
    "# Recursive Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZX4OI6Pswehg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkZpes7KwmRz",
    "outputId": "7c831a7e-ad60-473e-d277-575a0a0bcba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fa49YJhL3QFF"
   },
   "source": [
    "## Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgZo3giWvqwP",
    "outputId": "ab8ece24-6f3e-431f-c2a2-57be785ed6ec"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gdLXA-cLxCLJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw this movie in NEW York city. I was waiti...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a German film from 1974 that is someth...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I attempted watching this movie twice and even...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On his birthday a small boys tells his mother ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The person who wrote the review \"enough with t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message label\n",
       "0  I saw this movie in NEW York city. I was waiti...   neg\n",
       "1  This is a German film from 1974 that is someth...   neg\n",
       "2  I attempted watching this movie twice and even...   neg\n",
       "3  On his birthday a small boys tells his mother ...   neg\n",
       "4  The person who wrote the review \"enough with t...   pos"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = \"/content/drive/Shareddrives/G5/project-4-sentiment-classification/\"\n",
    "path = \"./\"\n",
    "train_data = pd.read_csv(path + \"train.csv\")\n",
    "test_data = pd.read_csv(path + \"test.csv\")\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bWYIUMPfxCkd",
    "outputId": "d9e701c1-a641-45a2-9ef8-8489d199a139"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jeffrey.monja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jeffrey.monja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jeffrey.monja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tokens'] = train_data['message'].apply(preprocess_text)\n",
    "test_data['tokens'] = test_data['message'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw this movie in NEW York city. I was waiti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>saw movie new york city waiting bus next morni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a German film from 1974 that is someth...</td>\n",
       "      <td>neg</td>\n",
       "      <td>german film something woman come castle beyond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I attempted watching this movie twice and even...</td>\n",
       "      <td>neg</td>\n",
       "      <td>attempted watching movie twice even fast forwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On his birthday a small boys tells his mother ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>birthday small boy tell mother son want go hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The person who wrote the review \"enough with t...</td>\n",
       "      <td>pos</td>\n",
       "      <td>person wrote review enough sweating spitting a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message label  \\\n",
       "0  I saw this movie in NEW York city. I was waiti...   neg   \n",
       "1  This is a German film from 1974 that is someth...   neg   \n",
       "2  I attempted watching this movie twice and even...   neg   \n",
       "3  On his birthday a small boys tells his mother ...   neg   \n",
       "4  The person who wrote the review \"enough with t...   pos   \n",
       "\n",
       "                                              tokens  \n",
       "0  saw movie new york city waiting bus next morni...  \n",
       "1  german film something woman come castle beyond...  \n",
       "2  attempted watching movie twice even fast forwa...  \n",
       "3  birthday small boy tell mother son want go hom...  \n",
       "4  person wrote review enough sweating spitting a...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()  \n",
    "train_data['label'] = label_encoder.fit_transform(train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = train_data['tokens'].apply(lambda x: x.split()).to_list()\n",
    "word2vec = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "sentences_test = test_data['tokens'].apply(lambda x: x.split()).to_list()\n",
    "word2vec_test = Word2Vec(sentences=sentences_test, vector_size=100, window=5, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creepier', 0.8047286868095398),\n",
       " ('freaked', 0.7866225242614746),\n",
       " ('scrat', 0.7834646701812744),\n",
       " ('preteen', 0.7825940251350403),\n",
       " ('ch', 0.7808871269226074)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(\"like\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vectors(sentence, model, vector_size=100):\n",
    "    vectors = []\n",
    "    for word in sentence.split():\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "        else:\n",
    "            vectors.append([0] * vector_size)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['vectors'] = train_data['tokens'].apply(lambda x: sentence_to_vectors(x, word2vec))\n",
    "test_data['vectors'] = test_data['tokens'].apply(lambda x: sentence_to_vectors(x, word2vec_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2), lowercase=True)\n",
    "train_data['tfidf'] = list(vectorizer.fit_transform(train_data['tokens']).toarray())\n",
    "test_data['tfidf'] = list(vectorizer.transform(test_data['tokens']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 1421\n",
      "Mean length: 119\n"
     ]
    }
   ],
   "source": [
    "max_len = max(train_data['tokens'].apply(lambda text: len(text.split())))\n",
    "mean_len = int(round(np.mean(train_data['tokens'].apply(lambda text: len(text.split())))))\n",
    "\n",
    "print(f'Max length: {max_len}')\n",
    "print(f'Mean length: {mean_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(vectors, max_len, vector_size=100):\n",
    "    if len(vectors) > max_len:\n",
    "        vectors = vectors[:max_len]\n",
    "    else:\n",
    "        padding = np.zeros((max_len - len(vectors), vector_size))\n",
    "        vectors = np.vstack([vectors, padding])\n",
    "    return torch.tensor(vectors, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tensor'] = train_data['vectors'].apply(lambda x: pad_sentences(x, mean_len, word2vec.vector_size))\n",
    "test_data['tensor'] = test_data['vectors'].apply(lambda x: pad_sentences(x, mean_len, word2vec_test.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>vectors</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw this movie in NEW York city. I was waiti...</td>\n",
       "      <td>0</td>\n",
       "      <td>saw movie new york city waiting bus next morni...</td>\n",
       "      <td>[[-0.21930268, 0.34450474, -0.6291782, -0.4775...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[tensor(-0.2193), tensor(0.3445), tensor(-0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a German film from 1974 that is someth...</td>\n",
       "      <td>0</td>\n",
       "      <td>german film something woman come castle beyond...</td>\n",
       "      <td>[[-0.33192742, 0.4755079, -0.21171649, -0.3054...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[tensor(-0.3319), tensor(0.4755), tensor(-0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I attempted watching this movie twice and even...</td>\n",
       "      <td>0</td>\n",
       "      <td>attempted watching movie twice even fast forwa...</td>\n",
       "      <td>[[-0.24511866, 0.021486083, 0.41561908, 0.0468...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[tensor(-0.2451), tensor(0.0215), tensor(0.41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On his birthday a small boys tells his mother ...</td>\n",
       "      <td>0</td>\n",
       "      <td>birthday small boy tell mother son want go hom...</td>\n",
       "      <td>[[0.18536536, 0.57395875, -0.559319, -0.181436...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[tensor(0.1854), tensor(0.5740), tensor(-0.55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The person who wrote the review \"enough with t...</td>\n",
       "      <td>1</td>\n",
       "      <td>person wrote review enough sweating spitting a...</td>\n",
       "      <td>[[-0.03453762, 0.2127353, 0.123608805, 0.44025...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[tensor(-0.0345), tensor(0.2127), tensor(0.12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  label  \\\n",
       "0  I saw this movie in NEW York city. I was waiti...      0   \n",
       "1  This is a German film from 1974 that is someth...      0   \n",
       "2  I attempted watching this movie twice and even...      0   \n",
       "3  On his birthday a small boys tells his mother ...      0   \n",
       "4  The person who wrote the review \"enough with t...      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  saw movie new york city waiting bus next morni...   \n",
       "1  german film something woman come castle beyond...   \n",
       "2  attempted watching movie twice even fast forwa...   \n",
       "3  birthday small boy tell mother son want go hom...   \n",
       "4  person wrote review enough sweating spitting a...   \n",
       "\n",
       "                                             vectors  \\\n",
       "0  [[-0.21930268, 0.34450474, -0.6291782, -0.4775...   \n",
       "1  [[-0.33192742, 0.4755079, -0.21171649, -0.3054...   \n",
       "2  [[-0.24511866, 0.021486083, 0.41561908, 0.0468...   \n",
       "3  [[0.18536536, 0.57395875, -0.559319, -0.181436...   \n",
       "4  [[-0.03453762, 0.2127353, 0.123608805, 0.44025...   \n",
       "\n",
       "                                               tfidf  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              tensor  \n",
       "0  [[tensor(-0.2193), tensor(0.3445), tensor(-0.6...  \n",
       "1  [[tensor(-0.3319), tensor(0.4755), tensor(-0.2...  \n",
       "2  [[tensor(-0.2451), tensor(0.0215), tensor(0.41...  \n",
       "3  [[tensor(0.1854), tensor(0.5740), tensor(-0.55...  \n",
       "4  [[tensor(-0.0345), tensor(0.2127), tensor(0.12...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 119, 100])\n",
      "torch.Size([25000])\n"
     ]
    }
   ],
   "source": [
    "X = torch.stack(train_data['tensor'].tolist())\n",
    "y = torch.tensor(train_data['label'].values)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))  # 80% para entrenamiento\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(len(train_loader)) \n",
    "print(len(test_loader)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZgX52qizr3j"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "w0khd9AlzZBt"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)  \n",
    "        hidden = hidden[-1]\n",
    "        out = self.fc(hidden)\n",
    "        return out\n",
    "\n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, window):\n",
    "#         _, (h_out, _) = self.lstm(window)\n",
    "#         h_out = h_out.view(-1, self.hidden_size)\n",
    "#         out = self.fc(h_out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Xt1QgN7q0DDd"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x) \n",
    "        hidden = hidden[-1] \n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "m7CfEr5y0Pgh"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(GRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hidden = self.gru(x)\n",
    "        hidden = hidden[-1]\n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBVBZnBm2JoO"
   },
   "source": [
    "## Training Function\n",
    "This function is designed to train all three models efficiently and streamline the process for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "B0DnyQLO2M2F"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_f, num_epochs, train_loader):\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            labels = labels.long()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_f(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaafyQ612rMq"
   },
   "source": [
    "## Hyperparameters\n",
    "Define the model's hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "3mYE5J-Z2wRh"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 100 # word2vec.vector_size\n",
    "hidden_size = 128\n",
    "output_size = 2\n",
    "num_layers = 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVpOmemB2zFO"
   },
   "source": [
    "## Create and train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx-9iIXkzy5H"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WseHCZ_J2352"
   },
   "outputs": [],
   "source": [
    "lstm = LSTM(input_size, hidden_size, output_size, num_layers)\n",
    "loss_function_LSTM = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "train(lstm, optimizer, loss_function_LSTM, num_epochs, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de búsqueda en cuadrícula de hiperparámetros\n",
    "def grid_search(param_grid):\n",
    "    best_accuracy = 0\n",
    "    best_params = {}\n",
    "\n",
    "    # Iterar sobre todas las combinaciones posibles de hiperparámetros\n",
    "    for hidden_size in param_grid['hidden_size']:\n",
    "        for num_layers in param_grid['num_layers']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                for batch_size in param_grid['batch_size']:\n",
    "                    for num_epochs in param_grid['num_epochs']:\n",
    "\n",
    "                        # Crear DataLoader con el tamaño de batch actual\n",
    "                        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                        # Inicializar el modelo LSTM con los parámetros actuales\n",
    "                        model = LSTM(input_size=100, hidden_size=hidden_size, output_size=2, num_layers=num_layers)\n",
    "\n",
    "                        # Inicializar la función de pérdida y el optimizador\n",
    "                        loss_function_LSTM = torch.nn.CrossEntropyLoss()\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                        # Entrenar el modelo\n",
    "                        print(f\"Entrenando con hidden_size={hidden_size}, num_layers={num_layers}, \"\n",
    "                              f\"learning_rate={learning_rate}, batch_size={batch_size}, num_epochs={num_epochs}\")\n",
    "                        train(model, optimizer, loss_function_LSTM, num_epochs, train_loader)\n",
    "\n",
    "                        # Evaluar el modelo\n",
    "                        accuracy = evaluate(model, test_loader)\n",
    "                        print(f\"Accuracy con hidden_size={hidden_size}, num_layers={num_layers}, \"\n",
    "                              f\"learning_rate={learning_rate}, batch_size={batch_size}, num_epochs={num_epochs}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "                        # Guardar la mejor configuración\n",
    "                        if accuracy > best_accuracy:\n",
    "                            best_accuracy = accuracy\n",
    "                            best_params = {\n",
    "                                'hidden_size': hidden_size,\n",
    "                                'num_layers': num_layers,\n",
    "                                'learning_rate': learning_rate,\n",
    "                                'batch_size': batch_size,\n",
    "                                'num_epochs': num_epochs\n",
    "                            }\n",
    "    \n",
    "    return best_accuracy, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=32, num_epochs=10\n",
      "Epoch 1/10, Loss: 0.6858070622444152\n",
      "Epoch 2/10, Loss: 0.6863924999237061\n",
      "Epoch 3/10, Loss: 0.6765423334121704\n",
      "Epoch 4/10, Loss: 0.6678766390323639\n",
      "Epoch 5/10, Loss: 0.6587735070228576\n",
      "Epoch 6/10, Loss: 0.6174238119602203\n",
      "Epoch 7/10, Loss: 0.40239698531627655\n",
      "Epoch 8/10, Loss: 0.3501253200292587\n",
      "Epoch 9/10, Loss: 0.33415063428878783\n",
      "Epoch 10/10, Loss: 0.3224551130056381\n",
      "Accuracy con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=32, num_epochs=10: 85.02%\n",
      "Entrenando con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=32, num_epochs=20\n",
      "Epoch 1/20, Loss: 0.6877557843208313\n",
      "Epoch 2/20, Loss: 0.6663573081970214\n",
      "Epoch 3/20, Loss: 0.6581019725322723\n",
      "Epoch 4/20, Loss: 0.6881966814041137\n",
      "Epoch 5/20, Loss: 0.6669482594490052\n",
      "Epoch 6/20, Loss: 0.6890221713066101\n",
      "Epoch 7/20, Loss: 0.5935684298038483\n",
      "Epoch 8/20, Loss: 0.3649332551121712\n",
      "Epoch 9/20, Loss: 0.331450732254982\n",
      "Epoch 10/20, Loss: 0.3231503041505814\n",
      "Epoch 11/20, Loss: 0.31393433418273925\n",
      "Epoch 12/20, Loss: 0.30944018267393114\n",
      "Epoch 13/20, Loss: 0.3076051345705986\n",
      "Epoch 14/20, Loss: 0.3028578362464905\n",
      "Epoch 15/20, Loss: 0.29818899935483933\n",
      "Epoch 16/20, Loss: 0.298762185382843\n",
      "Epoch 17/20, Loss: 0.29537526746988296\n",
      "Epoch 18/20, Loss: 0.2909606355667114\n",
      "Epoch 19/20, Loss: 0.28680993032455443\n",
      "Epoch 20/20, Loss: 0.28065141528844834\n",
      "Accuracy con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=32, num_epochs=20: 87.66%\n",
      "Entrenando con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=32, num_epochs=30\n",
      "Epoch 1/30, Loss: 0.6633960323810577\n",
      "Epoch 2/30, Loss: 0.6768710164546966\n",
      "Epoch 3/30, Loss: 0.6747811514377594\n",
      "Epoch 4/30, Loss: 0.4893863960266113\n",
      "Epoch 5/30, Loss: 0.353282830786705\n",
      "Epoch 6/30, Loss: 0.3324347101688385\n",
      "Epoch 7/30, Loss: 0.31887574343681335\n",
      "Epoch 8/30, Loss: 0.3134557554960251\n",
      "Epoch 9/30, Loss: 0.3060496371746063\n",
      "Epoch 10/30, Loss: 0.3025857154130936\n",
      "Epoch 11/30, Loss: 0.2957065620660782\n",
      "Epoch 12/30, Loss: 0.291980921792984\n",
      "Epoch 13/30, Loss: 0.28257893497943876\n",
      "Epoch 14/30, Loss: 0.27889279705286024\n",
      "Epoch 15/30, Loss: 0.2722112685084343\n",
      "Epoch 16/30, Loss: 0.2650292390525341\n",
      "Epoch 17/30, Loss: 0.25866677253246306\n",
      "Epoch 18/30, Loss: 0.2508711986422539\n",
      "Epoch 19/30, Loss: 0.24293088233470916\n",
      "Epoch 20/30, Loss: 0.2372481710255146\n",
      "Epoch 21/30, Loss: 0.22819335029125212\n",
      "Epoch 22/30, Loss: 0.22026839032769202\n",
      "Epoch 23/30, Loss: 0.21113775119781494\n",
      "Epoch 24/30, Loss: 0.20238425151109696\n",
      "Epoch 25/30, Loss: 0.1896792203307152\n",
      "Epoch 26/30, Loss: 0.18421343073248864\n",
      "Epoch 27/30, Loss: 0.17190575874447822\n",
      "Epoch 28/30, Loss: 0.163675785215199\n",
      "Epoch 29/30, Loss: 0.15215015195459128\n",
      "Epoch 30/30, Loss: 0.14452289690077305\n",
      "Accuracy con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=32, num_epochs=30: 86.10%\n",
      "Entrenando con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=64, num_epochs=10\n",
      "Epoch 1/10, Loss: 0.6718815603195288\n",
      "Epoch 2/10, Loss: 0.669875312441835\n",
      "Epoch 3/10, Loss: 0.6538737386750718\n",
      "Epoch 4/10, Loss: 0.6753223534590139\n",
      "Epoch 5/10, Loss: 0.6828488004855073\n",
      "Epoch 6/10, Loss: 0.6736686244940224\n",
      "Epoch 7/10, Loss: 0.6479475561993572\n",
      "Epoch 8/10, Loss: 0.6253482720341546\n",
      "Epoch 9/10, Loss: 0.6806322831315355\n",
      "Epoch 10/10, Loss: 0.64206545259625\n",
      "Accuracy con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=64, num_epochs=10: 74.64%\n",
      "Entrenando con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=64, num_epochs=20\n",
      "Epoch 1/20, Loss: 0.6853851742637805\n",
      "Epoch 2/20, Loss: 0.6806146626274425\n",
      "Epoch 3/20, Loss: 0.6648966987102557\n",
      "Epoch 4/20, Loss: 0.6928143942127594\n",
      "Epoch 5/20, Loss: 0.6836672122486103\n",
      "Epoch 6/20, Loss: 0.6917000879494908\n",
      "Epoch 7/20, Loss: 0.6854701611561517\n",
      "Epoch 8/20, Loss: 0.6707779353799911\n",
      "Epoch 9/20, Loss: 0.6815957372752242\n",
      "Epoch 10/20, Loss: 0.680924041393085\n",
      "Epoch 11/20, Loss: 0.611086879961026\n",
      "Epoch 12/20, Loss: 0.4160815626858903\n",
      "Epoch 13/20, Loss: 0.3512262678660524\n",
      "Epoch 14/20, Loss: 0.3314590513134917\n",
      "Epoch 15/20, Loss: 0.323362703378589\n",
      "Epoch 16/20, Loss: 0.3159577557549309\n",
      "Epoch 17/20, Loss: 0.31141913018097134\n",
      "Epoch 18/20, Loss: 0.3049773530076487\n",
      "Epoch 19/20, Loss: 0.3005020077140948\n",
      "Epoch 20/20, Loss: 0.296918587562756\n",
      "Accuracy con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=64, num_epochs=20: 85.74%\n",
      "Entrenando con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=64, num_epochs=30\n",
      "Epoch 1/30, Loss: 0.6845871808049017\n",
      "Epoch 2/30, Loss: 0.6746905910702178\n",
      "Epoch 3/30, Loss: 0.6432255616965005\n",
      "Epoch 4/30, Loss: 0.6429318708543199\n",
      "Epoch 5/30, Loss: 0.659517658404268\n",
      "Epoch 6/30, Loss: 0.6210291254253814\n",
      "Epoch 7/30, Loss: 0.6499693978327913\n",
      "Epoch 8/30, Loss: 0.5939690403092783\n",
      "Epoch 9/30, Loss: 0.6585317755849979\n",
      "Epoch 10/30, Loss: 0.6855384827421876\n",
      "Epoch 11/30, Loss: 0.6413668636887219\n",
      "Epoch 12/30, Loss: 0.6721102570573362\n",
      "Epoch 13/30, Loss: 0.5224612709432365\n",
      "Epoch 14/30, Loss: 0.3899487747361485\n",
      "Epoch 15/30, Loss: 0.36436035791144206\n",
      "Epoch 16/30, Loss: 0.3530644244564989\n",
      "Epoch 17/30, Loss: 0.3305196624023084\n",
      "Epoch 18/30, Loss: 0.3241717782549965\n",
      "Epoch 19/30, Loss: 0.3205738275195844\n",
      "Epoch 20/30, Loss: 0.31027698135985354\n",
      "Epoch 21/30, Loss: 0.31148345006730993\n",
      "Epoch 22/30, Loss: 0.30523559017874563\n",
      "Epoch 23/30, Loss: 0.3004953097849608\n",
      "Epoch 24/30, Loss: 0.30065041211561655\n",
      "Epoch 25/30, Loss: 0.29320219773263595\n",
      "Epoch 26/30, Loss: 0.29248486556850684\n",
      "Epoch 27/30, Loss: 0.2876449415858942\n",
      "Epoch 28/30, Loss: 0.2846115101545383\n",
      "Epoch 29/30, Loss: 0.2815788085706318\n",
      "Epoch 30/30, Loss: 0.278397447313554\n",
      "Accuracy con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=64, num_epochs=30: 87.94%\n",
      "Entrenando con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=128, num_epochs=10\n",
      "Epoch 1/10, Loss: 0.6829811729443301\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 11\u001b[0m\n\u001b[0;32m      2\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m],           \u001b[38;5;66;03m# Diferentes tamaños de la capa oculta\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m],                 \u001b[38;5;66;03m# Número de capas LSTM\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m]                   \u001b[38;5;66;03m# Número de épocas para entrenar\u001b[39;00m\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Realizar búsqueda en cuadrícula\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m best_accuracy, best_params \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Imprimir los mejores parámetros y la precisión\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMejores parámetros: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 27\u001b[0m, in \u001b[0;36mgrid_search\u001b[1;34m(param_grid)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntrenando con hidden_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_layers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_epochs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function_LSTM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Evaluar el modelo\u001b[39;00m\n\u001b[0;32m     30\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss_f, num_epochs, train_loader)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      8\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_f(outputs, labels)\n\u001b[0;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\jeffrey.monja\\Desktop\\Github\\2024-II\\ML\\project-4\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jeffrey.monja\\Desktop\\Github\\2024-II\\ML\\project-4\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[21], line 9\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     _, (hidden, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     10\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     11\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(hidden)\n",
      "File \u001b[1;32mc:\\Users\\jeffrey.monja\\Desktop\\Github\\2024-II\\ML\\project-4\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jeffrey.monja\\Desktop\\Github\\2024-II\\ML\\project-4\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\jeffrey.monja\\Desktop\\Github\\2024-II\\ML\\project-4\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parámetros para la búsqueda en cuadrícula\n",
    "param_grid = {\n",
    "    'hidden_size': [64, 128, 256],           # Diferentes tamaños de la capa oculta\n",
    "    'num_layers': [1, 2, 3],                 # Número de capas LSTM\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],# Diferentes tasas de aprendizaje\n",
    "    'batch_size': [32, 64, 128],             # Tamaño del batch\n",
    "    'num_epochs': [10, 20, 30]                   # Número de épocas para entrenar\n",
    "}\n",
    "\n",
    "# Realizar búsqueda en cuadrícula\n",
    "best_accuracy, best_params = grid_search(param_grid)\n",
    "\n",
    "# Imprimir los mejores parámetros y la precisión\n",
    "print(f\"\\nMejores parámetros: {best_params}\")\n",
    "print(f\"Mejor Accuracy: {best_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58-CAWLi0DZ_"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "nidEZSNC24h0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.6794653319835663\n",
      "Epoch 2/30, Loss: 0.6824531795978546\n",
      "Epoch 3/30, Loss: 0.6913336690902709\n",
      "Epoch 4/30, Loss: 0.6706020780563354\n",
      "Epoch 5/30, Loss: 0.6765170733451843\n",
      "Epoch 6/30, Loss: 0.6700386498451233\n",
      "Epoch 7/30, Loss: 0.6504225824832917\n",
      "Epoch 8/30, Loss: 0.6611987741470337\n",
      "Epoch 9/30, Loss: 0.6874834663391113\n",
      "Epoch 10/30, Loss: 0.6852647385597229\n",
      "Epoch 11/30, Loss: 0.6820053078651428\n",
      "Epoch 12/30, Loss: 0.6822574971199036\n",
      "Epoch 13/30, Loss: 0.6760899051666259\n",
      "Epoch 14/30, Loss: 0.6828932273864746\n",
      "Epoch 15/30, Loss: 0.6846454354286193\n",
      "Epoch 16/30, Loss: 0.6752873534202576\n",
      "Epoch 17/30, Loss: 0.6712932606697083\n",
      "Epoch 18/30, Loss: 0.6513241497516632\n",
      "Epoch 19/30, Loss: 0.6773011771202088\n",
      "Epoch 20/30, Loss: 0.6637817116737366\n",
      "Epoch 21/30, Loss: 0.6677394290924072\n",
      "Epoch 22/30, Loss: 0.6562525920391082\n",
      "Epoch 23/30, Loss: 0.6268605331897735\n",
      "Epoch 24/30, Loss: 0.6468837797641754\n",
      "Epoch 25/30, Loss: 0.5980380709171296\n",
      "Epoch 26/30, Loss: 0.6667880938529969\n",
      "Epoch 27/30, Loss: 0.655633044052124\n",
      "Epoch 28/30, Loss: 0.6136678344249725\n",
      "Epoch 29/30, Loss: 0.6397519834041595\n",
      "Epoch 30/30, Loss: 0.6261055971622467\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size, hidden_size, output_size, num_layers)\n",
    "loss_function_RNN = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "train(rnn, optimizer, loss_function_RNN, num_epochs, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-hX6-pp0OjD"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_BAHe7a25N8"
   },
   "outputs": [],
   "source": [
    "gru = GRU(input_size, hidden_size, output_size, num_layers)\n",
    "loss_function_GRU = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate)\n",
    "train(gru, optimizer, loss_function_GRU, num_epochs, train_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
