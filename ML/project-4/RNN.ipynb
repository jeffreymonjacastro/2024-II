{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQM6jtNKwFZq"
   },
   "source": [
    "# Recursive Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZX4OI6Pswehg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkZpes7KwmRz",
    "outputId": "7c831a7e-ad60-473e-d277-575a0a0bcba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fa49YJhL3QFF"
   },
   "source": [
    "## Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgZo3giWvqwP",
    "outputId": "ab8ece24-6f3e-431f-c2a2-57be785ed6ec"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gdLXA-cLxCLJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw this movie in NEW York city. I was waiti...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a German film from 1974 that is someth...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I attempted watching this movie twice and even...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On his birthday a small boys tells his mother ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The person who wrote the review \"enough with t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message label\n",
       "0  I saw this movie in NEW York city. I was waiti...   neg\n",
       "1  This is a German film from 1974 that is someth...   neg\n",
       "2  I attempted watching this movie twice and even...   neg\n",
       "3  On his birthday a small boys tells his mother ...   neg\n",
       "4  The person who wrote the review \"enough with t...   pos"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = \"/content/drive/Shareddrives/G5/project-4-sentiment-classification/\"\n",
    "path = \"./\"\n",
    "train_data = pd.read_csv(path + \"train.csv\")\n",
    "test_data = pd.read_csv(path + \"test.csv\")\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bWYIUMPfxCkd",
    "outputId": "d9e701c1-a641-45a2-9ef8-8489d199a139"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tokens'] = train_data['message'].apply(preprocess_text)\n",
    "test_data['tokens'] = test_data['message'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw this movie in NEW York city. I was waiti...</td>\n",
       "      <td>0</td>\n",
       "      <td>saw movie new york city waiting bus next morni...</td>\n",
       "      <td>[[-0.21996641, 0.17705941, -0.0033930233, -0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a German film from 1974 that is someth...</td>\n",
       "      <td>0</td>\n",
       "      <td>german film something woman come castle beyond...</td>\n",
       "      <td>[[-0.46728015, 0.27693665, -0.29466736, -0.378...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I attempted watching this movie twice and even...</td>\n",
       "      <td>0</td>\n",
       "      <td>attempted watching movie twice even fast forwa...</td>\n",
       "      <td>[[-0.12267326, -0.110086754, 0.07383823, 0.421...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On his birthday a small boys tells his mother ...</td>\n",
       "      <td>0</td>\n",
       "      <td>birthday small boy tell mother son want go hom...</td>\n",
       "      <td>[[0.0073563126, 0.34156787, -0.5304753, -0.314...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The person who wrote the review \"enough with t...</td>\n",
       "      <td>1</td>\n",
       "      <td>person wrote review enough sweating spitting a...</td>\n",
       "      <td>[[-0.0344274, 0.1957269, -0.32643205, 0.307864...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  label  \\\n",
       "0  I saw this movie in NEW York city. I was waiti...      0   \n",
       "1  This is a German film from 1974 that is someth...      0   \n",
       "2  I attempted watching this movie twice and even...      0   \n",
       "3  On his birthday a small boys tells his mother ...      0   \n",
       "4  The person who wrote the review \"enough with t...      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  saw movie new york city waiting bus next morni...   \n",
       "1  german film something woman come castle beyond...   \n",
       "2  attempted watching movie twice even fast forwa...   \n",
       "3  birthday small boy tell mother son want go hom...   \n",
       "4  person wrote review enough sweating spitting a...   \n",
       "\n",
       "                                             vectors  \n",
       "0  [[-0.21996641, 0.17705941, -0.0033930233, -0.6...  \n",
       "1  [[-0.46728015, 0.27693665, -0.29466736, -0.378...  \n",
       "2  [[-0.12267326, -0.110086754, 0.07383823, 0.421...  \n",
       "3  [[0.0073563126, 0.34156787, -0.5304753, -0.314...  \n",
       "4  [[-0.0344274, 0.1957269, -0.32643205, 0.307864...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()  \n",
    "train_data['label'] = label_encoder.fit_transform(train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = train_data['tokens'].apply(lambda x: x.split()).to_list()\n",
    "word2vec = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "sentences_test = test_data['tokens'].apply(lambda x: x.split()).to_list()\n",
    "word2vec_test = Word2Vec(sentences=sentences_test, vector_size=100, window=5, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creepier', 0.7893841862678528),\n",
       " ('hmmmm', 0.7754266262054443),\n",
       " ('preteen', 0.7654963135719299),\n",
       " ('alot', 0.7647897005081177),\n",
       " ('anyways', 0.7632929086685181)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(\"like\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vectors(sentence, model, vector_size=100):\n",
    "    vectors = []\n",
    "    for word in sentence.split():\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "        else:\n",
    "            vectors.append([0] * vector_size)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['vectors'] = train_data['tokens'].apply(lambda x: sentence_to_vectors(x, word2vec))\n",
    "test_data['vectors'] = test_data['tokens'].apply(lambda x: sentence_to_vectors(x, word2vec_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2), lowercase=True)\n",
    "train_data['tfidf'] = list(vectorizer.fit_transform(train_data['tokens']).toarray())\n",
    "test_data['tfidf'] = list(vectorizer.transform(test_data['tokens']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 1421\n",
      "Mean length: 119\n"
     ]
    }
   ],
   "source": [
    "max_len = max(train_data['tokens'].apply(lambda text: len(text.split())))\n",
    "mean_len = int(round(np.mean(train_data['tokens'].apply(lambda text: len(text.split())))))\n",
    "\n",
    "print(f'Max length: {max_len}')\n",
    "print(f'Mean length: {mean_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(vectors, max_len, vector_size=100):\n",
    "    if len(vectors) > max_len:\n",
    "        vectors = vectors[:max_len]\n",
    "    else:\n",
    "        padding = np.zeros((max_len - len(vectors), vector_size))\n",
    "        vectors = np.vstack([vectors, padding])\n",
    "    return torch.tensor(vectors, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tensor'] = train_data['vectors'].apply(lambda x: pad_sentences(x, mean_len, word2vec.vector_size))\n",
    "test_data['tensor'] = test_data['vectors'].apply(lambda x: pad_sentences(x, mean_len, word2vec_test.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>vectors</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw this movie in NEW York city. I was waiti...</td>\n",
       "      <td>0</td>\n",
       "      <td>saw movie new york city waiting bus next morni...</td>\n",
       "      <td>[[0.17053513, 0.044531778, -0.5490574, -0.4508...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[tensor(0.1705), tensor(0.0445), tensor(-0.54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a German film from 1974 that is someth...</td>\n",
       "      <td>0</td>\n",
       "      <td>german film something woman come castle beyond...</td>\n",
       "      <td>[[-0.2837934, 0.1716008, -0.071153946, -0.3870...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[tensor(-0.2838), tensor(0.1716), tensor(-0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I attempted watching this movie twice and even...</td>\n",
       "      <td>0</td>\n",
       "      <td>attempted watching movie twice even fast forwa...</td>\n",
       "      <td>[[-0.015660213, 0.0361225, 0.23330258, 0.34803...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[tensor(-0.0157), tensor(0.0361), tensor(0.23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On his birthday a small boys tells his mother ...</td>\n",
       "      <td>0</td>\n",
       "      <td>birthday small boy tell mother son want go hom...</td>\n",
       "      <td>[[0.16865695, 0.36055747, -0.49575418, -0.3732...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[tensor(0.1687), tensor(0.3606), tensor(-0.49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The person who wrote the review \"enough with t...</td>\n",
       "      <td>1</td>\n",
       "      <td>person wrote review enough sweating spitting a...</td>\n",
       "      <td>[[-0.079325594, 0.27384898, 0.07582834, 0.3497...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[tensor(-0.0793), tensor(0.2738), tensor(0.07...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  label  \\\n",
       "0  I saw this movie in NEW York city. I was waiti...      0   \n",
       "1  This is a German film from 1974 that is someth...      0   \n",
       "2  I attempted watching this movie twice and even...      0   \n",
       "3  On his birthday a small boys tells his mother ...      0   \n",
       "4  The person who wrote the review \"enough with t...      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  saw movie new york city waiting bus next morni...   \n",
       "1  german film something woman come castle beyond...   \n",
       "2  attempted watching movie twice even fast forwa...   \n",
       "3  birthday small boy tell mother son want go hom...   \n",
       "4  person wrote review enough sweating spitting a...   \n",
       "\n",
       "                                             vectors  \\\n",
       "0  [[0.17053513, 0.044531778, -0.5490574, -0.4508...   \n",
       "1  [[-0.2837934, 0.1716008, -0.071153946, -0.3870...   \n",
       "2  [[-0.015660213, 0.0361225, 0.23330258, 0.34803...   \n",
       "3  [[0.16865695, 0.36055747, -0.49575418, -0.3732...   \n",
       "4  [[-0.079325594, 0.27384898, 0.07582834, 0.3497...   \n",
       "\n",
       "                                               tfidf  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              tensor  \n",
       "0  [[tensor(0.1705), tensor(0.0445), tensor(-0.54...  \n",
       "1  [[tensor(-0.2838), tensor(0.1716), tensor(-0.0...  \n",
       "2  [[tensor(-0.0157), tensor(0.0361), tensor(0.23...  \n",
       "3  [[tensor(0.1687), tensor(0.3606), tensor(-0.49...  \n",
       "4  [[tensor(-0.0793), tensor(0.2738), tensor(0.07...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 119, 100])\n",
      "torch.Size([25000])\n"
     ]
    }
   ],
   "source": [
    "X = torch.stack(train_data['tensor'].tolist())\n",
    "y = torch.tensor(train_data['label'].values)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))  # 80% para entrenamiento\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(len(train_loader)) \n",
    "print(len(test_loader)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZgX52qizr3j"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0khd9AlzZBt"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)  \n",
    "        hidden = hidden[-1]\n",
    "        out = self.fc(hidden)\n",
    "        return out\n",
    "\n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, window):\n",
    "#         _, (h_out, _) = self.lstm(window)\n",
    "#         h_out = h_out.view(-1, self.hidden_size)\n",
    "#         out = self.fc(h_out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Xt1QgN7q0DDd"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x) \n",
    "        hidden = hidden[-1] \n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7CfEr5y0Pgh"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(GRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hidden = self.gru(x)\n",
    "        hidden = hidden[-1]\n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBVBZnBm2JoO"
   },
   "source": [
    "## Training Function\n",
    "This function is designed to train all three models efficiently and streamline the process for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "B0DnyQLO2M2F"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_f, num_epochs, train_loader):\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            labels = labels.long()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_f(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaafyQ612rMq"
   },
   "source": [
    "## Hyperparameters\n",
    "Define the model's hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mYE5J-Z2wRh"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 100 # word2vec.vector_size\n",
    "hidden_size = 128\n",
    "output_size = 2\n",
    "num_layers = 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVpOmemB2zFO"
   },
   "source": [
    "## Create and train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx-9iIXkzy5H"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "WseHCZ_J2352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.67310631275177\n",
      "Epoch 2/30, Loss: 0.6694829176902771\n",
      "Epoch 3/30, Loss: 0.5659720916748047\n",
      "Epoch 4/30, Loss: 0.347309819483757\n",
      "Epoch 5/30, Loss: 0.3212477997779846\n",
      "Epoch 6/30, Loss: 0.31245408944487574\n",
      "Epoch 7/30, Loss: 0.30451204755306244\n",
      "Epoch 8/30, Loss: 0.29581692311763763\n",
      "Epoch 9/30, Loss: 0.29111148651838303\n",
      "Epoch 10/30, Loss: 0.2821523575127125\n",
      "Epoch 11/30, Loss: 0.277583460944891\n",
      "Epoch 12/30, Loss: 0.2668392973065376\n",
      "Epoch 13/30, Loss: 0.25670862380862236\n",
      "Epoch 14/30, Loss: 0.2463035105586052\n",
      "Epoch 15/30, Loss: 0.23160683302283286\n",
      "Epoch 16/30, Loss: 0.2173728201806545\n",
      "Epoch 17/30, Loss: 0.2012839603126049\n",
      "Epoch 18/30, Loss: 0.1823293737858534\n",
      "Epoch 19/30, Loss: 0.16659079930782317\n",
      "Epoch 20/30, Loss: 0.14284013803899287\n",
      "Epoch 21/30, Loss: 0.12361272724866867\n",
      "Epoch 22/30, Loss: 0.10804368139356375\n",
      "Epoch 23/30, Loss: 0.09039054093137383\n",
      "Epoch 24/30, Loss: 0.07517291503101588\n",
      "Epoch 25/30, Loss: 0.06458085372969508\n",
      "Epoch 26/30, Loss: 0.055900429471954706\n",
      "Epoch 27/30, Loss: 0.050588812578096984\n",
      "Epoch 28/30, Loss: 0.048900693087466064\n",
      "Epoch 29/30, Loss: 0.036531493362784384\n",
      "Epoch 30/30, Loss: 0.039380745371151715\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(input_size, hidden_size, output_size, num_layers)\n",
    "loss_function_LSTM = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "train(lstm, optimizer, loss_function_LSTM, num_epochs, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de búsqueda en cuadrícula de hiperparámetros\n",
    "def grid_search(param_grid):\n",
    "    best_accuracy = 0\n",
    "    best_params = {}\n",
    "\n",
    "    # Iterar sobre todas las combinaciones posibles de hiperparámetros\n",
    "    for hidden_size in param_grid['hidden_size']:\n",
    "        for num_layers in param_grid['num_layers']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                for batch_size in param_grid['batch_size']:\n",
    "                    for num_epochs in param_grid['num_epochs']:\n",
    "\n",
    "                        # Crear DataLoader con el tamaño de batch actual\n",
    "                        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                        # Inicializar el modelo LSTM con los parámetros actuales\n",
    "                        model = LSTM(input_size=100, hidden_size=hidden_size, output_size=2, num_layers=num_layers)\n",
    "\n",
    "                        # Inicializar la función de pérdida y el optimizador\n",
    "                        loss_function_LSTM = torch.nn.CrossEntropyLoss()\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                        # Entrenar el modelo\n",
    "                        print(f\"Entrenando con hidden_size={hidden_size}, num_layers={num_layers}, \"\n",
    "                              f\"learning_rate={learning_rate}, batch_size={batch_size}, num_epochs={num_epochs}\")\n",
    "                        train(model, optimizer, loss_function_LSTM, num_epochs, train_loader)\n",
    "\n",
    "                        # Evaluar el modelo\n",
    "                        accuracy = evaluate(model, test_loader)\n",
    "                        print(f\"Accuracy con hidden_size={hidden_size}, num_layers={num_layers}, \"\n",
    "                              f\"learning_rate={learning_rate}, batch_size={batch_size}, num_epochs={num_epochs}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "                        # Guardar la mejor configuración\n",
    "                        if accuracy > best_accuracy:\n",
    "                            best_accuracy = accuracy\n",
    "                            best_params = {\n",
    "                                'hidden_size': hidden_size,\n",
    "                                'num_layers': num_layers,\n",
    "                                'learning_rate': learning_rate,\n",
    "                                'batch_size': batch_size,\n",
    "                                'num_epochs': num_epochs\n",
    "                            }\n",
    "    \n",
    "    return best_accuracy, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=32, num_epochs=10\n",
      "Epoch 1/10, Loss: 0.6768471588134766\n",
      "Epoch 2/10, Loss: 0.6823424056053161\n",
      "Epoch 3/10, Loss: 0.6500057030677795\n",
      "Epoch 4/10, Loss: 0.6567579681396485\n",
      "Epoch 5/10, Loss: 0.6770723893642425\n",
      "Epoch 6/10, Loss: 0.547683597946167\n",
      "Epoch 7/10, Loss: 0.3490979295015335\n",
      "Epoch 8/10, Loss: 0.3291783967256546\n",
      "Epoch 9/10, Loss: 0.3170449034690857\n",
      "Epoch 10/10, Loss: 0.3127932381272316\n",
      "Accuracy con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=32, num_epochs=10: 86.82%\n",
      "Entrenando con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=32, num_epochs=20\n",
      "Epoch 1/20, Loss: 0.6727185891628266\n",
      "Epoch 2/20, Loss: 0.6612150899410247\n",
      "Epoch 3/20, Loss: 0.6191956439495087\n",
      "Epoch 4/20, Loss: 0.6384363409042358\n",
      "Epoch 5/20, Loss: 0.6589520169258117\n",
      "Epoch 6/20, Loss: 0.642452643918991\n",
      "Epoch 7/20, Loss: 0.5970067672729492\n",
      "Epoch 8/20, Loss: 0.4220517461776733\n",
      "Epoch 9/20, Loss: 0.3469949995517731\n",
      "Epoch 10/20, Loss: 0.32385805060863493\n",
      "Epoch 11/20, Loss: 0.314353445827961\n",
      "Epoch 12/20, Loss: 0.30507747546434405\n",
      "Epoch 13/20, Loss: 0.2969329825639725\n",
      "Epoch 14/20, Loss: 0.29179721775054934\n",
      "Epoch 15/20, Loss: 0.2854685559272766\n",
      "Epoch 16/20, Loss: 0.27809575462341307\n",
      "Epoch 17/20, Loss: 0.27202391927838326\n",
      "Epoch 18/20, Loss: 0.2630403697490692\n",
      "Epoch 19/20, Loss: 0.2569165784239769\n",
      "Epoch 20/20, Loss: 0.2450114258348942\n",
      "Accuracy con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=32, num_epochs=20: 86.56%\n",
      "Entrenando con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=64, num_epochs=10\n",
      "Epoch 1/10, Loss: 0.6826403666609011\n",
      "Epoch 2/10, Loss: 0.6775995903312207\n",
      "Epoch 3/10, Loss: 0.6403536827990803\n",
      "Epoch 4/10, Loss: 0.6813698180567342\n",
      "Epoch 5/10, Loss: 0.6647973099646096\n",
      "Epoch 6/10, Loss: 0.6815001622746928\n",
      "Epoch 7/10, Loss: 0.6770501610950921\n",
      "Epoch 8/10, Loss: 0.6752651265254036\n",
      "Epoch 9/10, Loss: 0.49671486210518373\n",
      "Epoch 10/10, Loss: 0.36602366660920954\n",
      "Accuracy con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=64, num_epochs=10: 86.02%\n",
      "Entrenando con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=64, num_epochs=20\n",
      "Epoch 1/20, Loss: 0.6791922067300961\n",
      "Epoch 2/20, Loss: 0.6849734305192868\n",
      "Epoch 3/20, Loss: 0.6842821067133651\n",
      "Epoch 4/20, Loss: 0.6738817423296432\n",
      "Epoch 5/20, Loss: 0.6733981330935567\n",
      "Epoch 6/20, Loss: 0.6685637419406598\n",
      "Epoch 7/20, Loss: 0.6745675813656645\n",
      "Epoch 8/20, Loss: 0.6744519830130922\n",
      "Epoch 9/20, Loss: 0.6423364964346535\n",
      "Epoch 10/20, Loss: 0.5181681708025094\n",
      "Epoch 11/20, Loss: 0.41486167265013\n",
      "Epoch 12/20, Loss: 0.35294868951788344\n",
      "Epoch 13/20, Loss: 0.33713877234405604\n",
      "Epoch 14/20, Loss: 0.32384212138934637\n",
      "Epoch 15/20, Loss: 0.31525532429972397\n",
      "Epoch 16/20, Loss: 0.30625916503298395\n",
      "Epoch 17/20, Loss: 0.30192004148952495\n",
      "Epoch 18/20, Loss: 0.296881894096018\n",
      "Epoch 19/20, Loss: 0.2921442837475207\n",
      "Epoch 20/20, Loss: 0.2916394586856373\n",
      "Accuracy con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=64, num_epochs=20: 86.66%\n",
      "Entrenando con hidden_size=64, num_layers=1, learning_rate=0.001, batch_size=128, num_epochs=10\n",
      "Epoch 1/10, Loss: 0.6805000825292745\n",
      "Epoch 2/10, Loss: 0.6428536089362612\n",
      "Epoch 3/10, Loss: 0.6501961172006692\n",
      "Epoch 4/10, Loss: 0.6452169873911864\n",
      "Epoch 5/10, Loss: 0.6857485721825035\n",
      "Epoch 6/10, Loss: 0.62172447799877\n",
      "Epoch 7/10, Loss: 0.5581894473285433\n",
      "Epoch 8/10, Loss: 0.6493773185143805\n"
     ]
    }
   ],
   "source": [
    "# Parámetros para la búsqueda en cuadrícula\n",
    "param_grid = {\n",
    "    'hidden_size': [64, 128, 256],           # Diferentes tamaños de la capa oculta\n",
    "    'num_layers': [1, 2, 3],                 # Número de capas LSTM\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],# Diferentes tasas de aprendizaje\n",
    "    'batch_size': [32, 64, 128],             # Tamaño del batch\n",
    "    'num_epochs': [10, 20]                   # Número de épocas para entrenar\n",
    "}\n",
    "\n",
    "# Realizar búsqueda en cuadrícula\n",
    "best_accuracy, best_params = grid_search(param_grid)\n",
    "\n",
    "# Imprimir los mejores parámetros y la precisión\n",
    "print(f\"\\nMejores parámetros: {best_params}\")\n",
    "print(f\"Mejor Accuracy: {best_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58-CAWLi0DZ_"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "nidEZSNC24h0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.6794653319835663\n",
      "Epoch 2/30, Loss: 0.6824531795978546\n",
      "Epoch 3/30, Loss: 0.6913336690902709\n",
      "Epoch 4/30, Loss: 0.6706020780563354\n",
      "Epoch 5/30, Loss: 0.6765170733451843\n",
      "Epoch 6/30, Loss: 0.6700386498451233\n",
      "Epoch 7/30, Loss: 0.6504225824832917\n",
      "Epoch 8/30, Loss: 0.6611987741470337\n",
      "Epoch 9/30, Loss: 0.6874834663391113\n",
      "Epoch 10/30, Loss: 0.6852647385597229\n",
      "Epoch 11/30, Loss: 0.6820053078651428\n",
      "Epoch 12/30, Loss: 0.6822574971199036\n",
      "Epoch 13/30, Loss: 0.6760899051666259\n",
      "Epoch 14/30, Loss: 0.6828932273864746\n",
      "Epoch 15/30, Loss: 0.6846454354286193\n",
      "Epoch 16/30, Loss: 0.6752873534202576\n",
      "Epoch 17/30, Loss: 0.6712932606697083\n",
      "Epoch 18/30, Loss: 0.6513241497516632\n",
      "Epoch 19/30, Loss: 0.6773011771202088\n",
      "Epoch 20/30, Loss: 0.6637817116737366\n",
      "Epoch 21/30, Loss: 0.6677394290924072\n",
      "Epoch 22/30, Loss: 0.6562525920391082\n",
      "Epoch 23/30, Loss: 0.6268605331897735\n",
      "Epoch 24/30, Loss: 0.6468837797641754\n",
      "Epoch 25/30, Loss: 0.5980380709171296\n",
      "Epoch 26/30, Loss: 0.6667880938529969\n",
      "Epoch 27/30, Loss: 0.655633044052124\n",
      "Epoch 28/30, Loss: 0.6136678344249725\n",
      "Epoch 29/30, Loss: 0.6397519834041595\n",
      "Epoch 30/30, Loss: 0.6261055971622467\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size, hidden_size, output_size, num_layers)\n",
    "loss_function_RNN = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "train(rnn, optimizer, loss_function_RNN, num_epochs, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-hX6-pp0OjD"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_BAHe7a25N8"
   },
   "outputs": [],
   "source": [
    "gru = GRU(input_size, hidden_size, output_size, num_layers)\n",
    "loss_function_GRU = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate)\n",
    "train(gru, optimizer, loss_function_GRU, num_epochs, train_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
