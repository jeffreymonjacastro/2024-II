{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUGj2nLirz-v"
      },
      "source": [
        " # Practice  Soft SVM.\n",
        " ----\n",
        "  \n",
        "  University : UTEC \\\\\n",
        "  Course       : Machine Learning \\\\\n",
        "  Professor    : Cristian López Del Alamo \\\\\n",
        "  Topic        : Soft SVM \\\\\n",
        "  Termina      : 4:45\n",
        "   \n",
        "\n",
        " ----\n",
        "\n",
        "Write the names and surnames of the members and the percentage of participation of each one in the development of the practice:\n",
        " - Integrante 1: (%)\n",
        " - Integrante 2: (%)\n",
        " - Integrante 3: (%)\n",
        " - Integrante 4: (%)\n",
        "\n",
        "\n",
        " ----\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgBNS0vz9yIW",
        "outputId": "96c06cf6-4ac4-4fae-e4c2-6bc4d51f172c"
      },
      "outputs": [],
      "source": [
        "# Loading Libraries\n",
        "# %pip install pywavelets\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import cvxopt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import pywt\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import zipfile\n",
        "from skimage import io, color"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBLjkIM3Ylev"
      },
      "source": [
        "## Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Normalizing\n",
        "def normalizacion(data):\n",
        "  \"\"\"\n",
        "  Normaliza los datos para que tengan una media de 0 y una desviación estándar de 1.\n",
        "  \n",
        "  Parameters:\n",
        "  data (numpy.ndarray): Un array de numpy con los datos a normalizar.\n",
        "  \n",
        "  Returns:\n",
        "  numpy.ndarray: Los datos normalizados.\n",
        "  \"\"\"\n",
        "  mean = np.mean(data, axis=0)\n",
        "  std = np.std(data, axis=0)\n",
        "  normalized_data = (data - mean) / std\n",
        "  return normalized_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbEqB9ZOYifu"
      },
      "source": [
        "## 1. Feature Vector\n",
        "\n",
        "A wavelet is a more efficient method than Fourier transforms for approximating a non-periodic function. Since an image can be seen as a two-dimensional function in a 3D space, we can approximate it using wavelets. In this case, we use the Hard Wavelet. The goal is to obtain a characteristic vector that represents each image\n",
        "\n",
        "[library:](https://pywavelets.readthedocs.io/en/latest/index.html)\n",
        "----\n",
        "Parámetros de la Función **haar**\n",
        "- f: Image File Name\n",
        "- n: Number of slider in the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5mKMHnw3HJek"
      },
      "outputs": [],
      "source": [
        "# Wavelet function for extracting image characteristics. This function returns a vector with image characteristics\n",
        "def haar(f, n):\n",
        "  imagen = io.imread(f)\n",
        "  for i in range(n):\n",
        "    imagen, (LH, HL, HH) = pywt.dwt2(imagen, 'haar')\n",
        "  gfg = np.array(imagen)\n",
        "  return gfg.flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFPIA4MqrYka"
      },
      "source": [
        "### Encode Function\n",
        "Encode is a function that encodes images into feature vectors using wavelets to extract relevant information from each image. Encode takes the path where the images are located, encodes them, and returns a matrix where the ith element of the matrix is the feature vector of image i."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BzY81s98C70o"
      },
      "outputs": [],
      "source": [
        "# This function loads all images in the specified path and returns a vector containing the feature vectors of these images. In the example, we use three splits.\n",
        "def encode(path, cuts):\n",
        "  encodings = []\n",
        "\n",
        "  print(\"Ingresó : \")\n",
        "  print(path)\n",
        "  i=0\n",
        "  for filename in os.listdir(path):\n",
        "      image_file = os.path.join(path, filename)\n",
        "      print(image_file)\n",
        "      if os.path.isfile(image_file):\n",
        "          encodings.append(haar(image_file, cuts))\n",
        "      i=i+1\n",
        "      if i==20:\n",
        "        return encodings\n",
        "  return encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm-Xm5NPS-qT"
      },
      "source": [
        "## Loading the data and extracting features using Hard Wavelet\n",
        "\n",
        "### get_data Function\n",
        "This function takes as input the paths where images of happy and disgust people are located, encodes them, and stores them in matrices. One matrix contains rows with features of happy people and another matrix contains features of sad people; both are combined into the same matrix. The rows of the matrix are randomized to then create the database that will train our soft SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "32eJou8tjAlL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_data(emotion_1, emotion_2, cuts):\n",
        "  data  =  np.array(encode(emotion_1,cuts))\n",
        "  data  = np.insert(data, 0, 1, axis=1)\n",
        "  temp =  np.array(encode(emotion_2,cuts))\n",
        "  temp = np.insert(temp, 0, -1, axis=1)\n",
        "  data = np.concatenate((data, temp), axis=0)\n",
        "  for i in range(10):\n",
        "    np.random.shuffle(data)\n",
        "  y = data[:,0]\n",
        "  x = data[:, 1:]\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oPw5bMztgsD"
      },
      "source": [
        "# Load train and test data\n",
        "\n",
        "We mount the drive in Colab, get the path where our images are located, and load the test and training data\n",
        "\n",
        "[Download DataSet](https://drive.google.com/drive/folders/1TvgyPE5l6TWtx7Fa6ZSKVUG2SsvEvh7l?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzOT129ul4pm",
        "outputId": "dac1946d-3781-4563-cad8-d25ae982cbf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "path: /home/jamcy/Github/2024-II/ML/W5\n",
            "Ingresó : \n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/7224.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/3717.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/13654.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/11767.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/12430.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/4891.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/32706.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/9227.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/23597.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/29436.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/110.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/14013.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/23830.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/22257.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/23185.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/21373.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/24223.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/24243.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/10869.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/fear/26805.jpg\n",
            "Ingresó : \n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/22443.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/22813.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/16671.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/8234.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/1256.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/11224.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/4286.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/4270.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/15215.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/15901.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/21401.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/17978.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/32840.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/27627.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/21832.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/10748.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/12736.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/11378.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/14264.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/train/neutral/13336.jpg\n",
            "Ingresó : \n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/23731.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/414.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/23540.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/12496.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/1169.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/27094.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/10534.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/34143.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/30638.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/911.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/32696.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/6747.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/14057.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/19559.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/9101.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/148.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/26131.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/35427.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/13694.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/fear/10832.jpg\n",
            "Ingresó : \n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/23039.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/1220.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/23741.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/5831.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/16569.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/28025.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/16350.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/31478.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/28278.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/32671.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/13161.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/2172.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/15963.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/10774.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/19855.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/11818.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/3204.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/2953.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/5695.jpg\n",
            "/home/jamcy/Github/2024-II/ML/W5/images/validation/neutral/29556.jpg\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# #  In the variable path, write the path where your dataset is located\n",
        "# path = '/content/drive/MyDrive/UTEC/CURSOS/2022.2/Inteligencia Artificial/DataSet/images'\n",
        "print(\"path:\", os.getcwd())\n",
        "path = os.getcwd() + \"/images\"\n",
        "\n",
        "c1_emotion = 'fear'\n",
        "c2_emotion = 'neutral'\n",
        "\n",
        "c1_emotion_train  = path +  '/train/' + c1_emotion\n",
        "c2_emotion_train  = path +  '/train/' + c2_emotion\n",
        "\n",
        "c1_emotion_test  = path +  '/validation/' + c1_emotion\n",
        "c2_emotion_test  = path +  '/validation/' + c2_emotion\n",
        "\n",
        "train_x, train_y = get_data(c1_emotion_train, c2_emotion_train,1)\n",
        "test_x, test_y = get_data(c1_emotion_test, c2_emotion_test,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH1duP_rs5kv",
        "outputId": "a19800be-9dbd-43e2-c885-3efb68f77ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40, 576)\n",
            "(40,)\n",
            "(40, 576)\n",
            "(40,)\n"
          ]
        }
      ],
      "source": [
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf3IPJbCeP3z"
      },
      "source": [
        "### 1. Hypothesis :  Remember, you can achieve it using linear algebra.\n",
        "\n",
        "$h(x_i) = x_i*w^t + b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "sAl5uDMceQSk"
      },
      "outputs": [],
      "source": [
        "def h(X,w,b):\n",
        "  \"\"\"\n",
        "  Hipótesis de la SVM\n",
        "\n",
        "  Parameters:\n",
        "  X -- Matriz de datos de tamaño (m, n)\n",
        "  w -- Vector de pesos de tamaño (1, n)\n",
        "  b -- Bias\n",
        "\n",
        "  Returns: \n",
        "  numpy.ndarray -- Vector de tamaño (m, 1) con las predicciones de la SVM\n",
        "  \"\"\"\n",
        "  return np.dot(X, w) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyZEBRyKS5j_"
      },
      "source": [
        "### 2. Loss function\n",
        "\n",
        "$L = \\frac{1}{2}\\parallel W \\parallel_2^2  + C\\sum_{i=0}^nmax(0,1-y_i(x_iw^t + b)) $\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "d6uS2nT1vqJw"
      },
      "outputs": [],
      "source": [
        "# Implemente la función de pérdida del soft SVM\n",
        "def loss(y,y_aprox,W,C):\n",
        "  \"\"\"\n",
        "  Función de pérdida de la SVM\n",
        "\n",
        "  Parameters:\n",
        "  y -- Vector de etiquetas de tamaño (m, 1)\n",
        "  y_aprox -- Vector de predicciones de tamaño (m, 1)\n",
        "  W -- Vector de pesos de tamaño (1, n)\n",
        "  C -- Parámetro de regularización\n",
        "\n",
        "  Returns:\n",
        "  float -- Valor de la función de pérdida\n",
        "  \"\"\"\n",
        "  \n",
        "  #Regularization  first  2 items\n",
        "  regularization_term = 1/2 * (np.linalg.norm(W)**2)\n",
        "\n",
        "  #Loss Term\n",
        "  h_loss = np.maximum(0, 1 - y * y_aprox)\n",
        "  h_loss_sum = C * np.sum(h_loss)\n",
        "\n",
        "  #Finding the total loss\n",
        "  total_loss = regularization_term + h_loss_sum\n",
        "  return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtQf5LcKsRmj"
      },
      "source": [
        "### 3. Derivatives\n",
        "\n",
        "1. iF ($y_i*h(x_i)) \\lt 1 $\n",
        " - $\\frac{\\partial L}{\\partial w} = w + C\\sum_{\n",
        " i=0}^n-y_i*x_i $\n",
        "2. else\n",
        " - $\\frac{\\partial L}{\\partial w} = w $\n",
        "\n",
        "----\n",
        "**Don't forget to find the derivatives for Bias.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "6GWcF6a-v0J1"
      },
      "outputs": [],
      "source": [
        "# Implemente la función para obtener las derivadas de W\n",
        "def derivatives(x, y,y_aprox, w, b, C):\n",
        "  \"\"\"\n",
        "  Derivadas de la función de pérdida de la SVM\n",
        "\n",
        "  Parameters:\n",
        "  x -- Matriz de datos de tamaño (m, n)\n",
        "  y -- Vector de etiquetas de tamaño (m, 1)\n",
        "  y_aprox -- Vector de predicciones de tamaño (m, 1)\n",
        "  w -- Vector de pesos de tamaño (1, n)\n",
        "  b -- Bias\n",
        "  C -- Parámetro de regularización\n",
        "\n",
        "  Returns:\n",
        "  numpy.ndarray -- Vector de tamaño (1, n) con las derivadas de W\n",
        "  \"\"\"\n",
        "  \n",
        "  # Condición para cada elemento\n",
        "  condition = (y * y_aprox) < 1\n",
        "  \n",
        "  # Derivadas de w y b\n",
        "  dw = np.where(condition[:, np.newaxis], w + C * (-y[:, np.newaxis] * x), w)\n",
        "  db = np.where(condition, -C * y, 0)\n",
        "  \n",
        "  # Sumar las derivadas a lo largo de las filas\n",
        "  dw = np.sum(dw, axis=0)\n",
        "  db = np.sum(db, axis=0)\n",
        "  \n",
        "  return db, dw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dKoV2jksZ0-"
      },
      "source": [
        "### 4. Change Parameters\n",
        "\n",
        "\n",
        "1. iF ($y_i*h(x_i)) \\lt 1 $\n",
        " - $w  = w -  \\alpha( C\\sum_{\n",
        " i=0}^n-y_i*x_i) $\n",
        "2. else\n",
        " - $w = w -  \\alpha*w$\n",
        "\n",
        "----\n",
        "**Don't forget charge the Bias.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "G6fVT25-v579"
      },
      "outputs": [],
      "source": [
        "def Update(x,y,y_aprox,w, b, db, dw, alpha):\n",
        "  \"\"\"\n",
        "  Función para actualizar los pesos de la SVM\n",
        "\n",
        "  Parameters:\n",
        "  x -- Matriz de datos de tamaño (m, n)\n",
        "  y -- Vector de etiquetas de tamaño (m, 1)\n",
        "  y_aprox -- Vector de predicciones de tamaño (m, 1)\n",
        "  w -- Vector de pesos de tamaño (1, n)\n",
        "  b -- Bias\n",
        "  db -- Derivada de b\n",
        "  dw -- Derivada de w\n",
        "  alpha -- Tasa de aprendizaje\n",
        "\n",
        "  Returns:\n",
        "  w -- Vector de tamaño (1, n) con los pesos actualizados\n",
        "  b -- Bias actualizado\n",
        "  \"\"\"\n",
        "\n",
        "  # Condición para cada elemento\n",
        "  condition = (y * y_aprox) < 1\n",
        "  \n",
        "  # Actualización de w y b\n",
        "  w = np.where(condition[:, np.newaxis], w - alpha * dw, w - alpha * w)\n",
        "  b = np.where(condition, b - alpha * db, b)\n",
        "\n",
        "  return w, b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEKGyLYJsfDc"
      },
      "source": [
        "# 5. Training stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "K9lvycWyv-Qd"
      },
      "outputs": [],
      "source": [
        "def training(X, Y, C, alpha, epochs):\n",
        "  w = np.array([np.random.rand() for i in range(X.shape[1])])\n",
        "  b = np.random.rand()\n",
        "  error = []\n",
        "  for i in range(epochs):\n",
        "    Y_aprox = h(X,w,b)\n",
        "    db, dw = derivatives(X, Y,Y_aprox, w, b, C)\n",
        "    w, b = Update(X,Y,Y_aprox,w, b, db, dw, alpha)\n",
        "    L = loss(Y,Y_aprox,w,C)\n",
        "    error.append(L)\n",
        "  return w, b, error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJRNAmzbsjKp"
      },
      "source": [
        "### 6. Testing Stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "I0iqS5EdwEM_"
      },
      "outputs": [],
      "source": [
        "# Implemente la función de testing\n",
        "def testing(X,W,b):\n",
        "  y_aprox = []\n",
        "  # write your code here\n",
        "  for i in range(X.shape[0]):\n",
        "    y_aprox = np.sign(h(X[i], W, b))\n",
        "  return np.array(y_aprox)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "0uF_jWeeTAPc"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "shapes (40,576) and (40,576) not aligned: 576 (dim 1) != 40 (dim 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[69], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m train_x_norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(normalizacion, \u001b[38;5;241m1\u001b[39m, train_x)\n\u001b[1;32m      6\u001b[0m test_x_norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(normalizacion, \u001b[38;5;241m1\u001b[39m, test_x)\n\u001b[0;32m----> 8\u001b[0m W, b, e1, \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m m \u001b[38;5;241m=\u001b[39m test_y\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m     10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m testing(test_x_norm, W, b)\n",
            "Cell \u001b[0;32mIn[58], line 6\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(X, Y, C, alpha, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m error \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 6\u001b[0m   Y_aprox \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m   db, dw \u001b[38;5;241m=\u001b[39m derivatives(X, Y,Y_aprox, w, b, C)\n\u001b[1;32m      8\u001b[0m   w, b \u001b[38;5;241m=\u001b[39m Update(X,Y,Y_aprox,w, b, db, dw, alpha)\n",
            "Cell \u001b[0;32mIn[60], line 13\u001b[0m, in \u001b[0;36mh\u001b[0;34m(X, w, b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mh\u001b[39m(X,w,b):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m  Hipótesis de la SVM\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m  numpy.ndarray -- Vector de tamaño (m, 1) con las predicciones de la SVM\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (40,576) and (40,576) not aligned: 576 (dim 1) != 40 (dim 0)"
          ]
        }
      ],
      "source": [
        "# Main Program\n",
        "\n",
        "m = train_y.size\n",
        "k = train_x[0].size\n",
        "train_x_norm = np.apply_along_axis(normalizacion, 1, train_x)\n",
        "test_x_norm = np.apply_along_axis(normalizacion, 1, test_x)\n",
        "\n",
        "W, b, e1, = training(train_x_norm, train_y, 1e6, 1e-8, 1200)\n",
        "m = test_y.size\n",
        "y_pred = testing(test_x_norm, W, b)\n",
        "test_y = test_y.astype('int')\n",
        "\n",
        "print(\"Clasificados correctamente:\", correct)\n",
        "print(\"Clasificados incorrectamente:\", len(test_y)-correct)\n",
        "print(\"% de efectividad\", round(100*correct/len(test_y), 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du50NSwaKe83"
      },
      "outputs": [],
      "source": [
        "matrix = confusion_matrix(test_y,y_pred)\n",
        "df2 = pd.DataFrame(matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis], index=[\"Angry\", 'Happy'], columns=[\"Angry\", 'Happy'])\n",
        "sns.heatmap(df2, annot=True, cbar=None, cmap=\"Greens\")\n",
        "plt.title(\"Confusion Matrix\"), plt.tight_layout()\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hkcd0hls1Ir"
      },
      "source": [
        "[Documento de excel](https://docs.google.com/spreadsheets/d/1yxYCjj_uS2Wrj8ofAc7hiRoCYX0-wJNiTMzGnWdsZoA/edit?usp=sharing)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
